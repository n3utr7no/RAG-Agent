{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG Application with IBM watsonx.ai and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"protobuf==4.21.12\"\n",
    "\n",
    "%pip install -U \"langchain>=0.3,<0.4\" \\\n",
    "                 \"langchain_ibm>=0.3,<0.4\" \\\n",
    "                 \"langchain_community>=0.3,<0.4\" \\\n",
    "                 \"langchain_chroma>=0.2,<0.3\" \\\n",
    "                 sentence-transformers \\\n",
    "                 wget \\\n",
    "                 chromadb \\\n",
    "                 ibm-watsonx-ai\n",
    "\n",
    "%pip install ibm-watson-machine-learning==1.0.312\n",
    "%pip install \"pandas==2.1.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring IBM watsonx.ai Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from ibm_watsonx_ai import Credentials\n",
    "\n",
    "# The URL for the watsonx.ai API endpoint\n",
    "url = 'https://us-south.ml.cloud.ibm.com'\n",
    "\n",
    "# Enter the API key\n",
    "api_key = getpass.getpass(\"Please enter your WML api key (hit enter): \")\n",
    "\n",
    "# pass the credentials as an object\n",
    "credentials = Credentials(\n",
    "    api_key=api_key,\n",
    "    url=url\n",
    ")\n",
    "\n",
    "# Getting the project ID\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    print(\"Downloading the data file...\")\n",
    "    wget.download(url, out=filename)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Data file already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Document split into {len(texts)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initializing the embedding model from watsonx.ai\n",
    "embeddings = Embeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "# Creating the Chroma vector store by embedding the text chunks\n",
    "print(\"Creating vector store...\")\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "print(\"Vector store created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 150, # Increased for potentially more detailed answers\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
    "}\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building and Running the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# The 'stuff' chain_type is the simplest, as it \"stuffs\" all retrieved documents into the final prompt.\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=watsonx_granite, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What did the president say about Ketanji Brown Jackson\n",
      "----------------------------------------------------------------\n",
      "Response:  The president said that Ketanji Brown Jackson is one of our nation's top legal minds and that she will continue Justice Breyer's legacy of excellence. He also mentioned that she is a former top litigator in private practice and a former federal public defender.\n"
     ]
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "response = qa.invoke(query) # Using .invoke() which is the standard for new LangChain versions\n",
    "\n",
    "print(f\"Query: {response['query']}\\n\")\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "print(f\"Response: {response['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = \"What are the pillars of the Unity Agenda?\" # Change this to your question\n",
    "my_response = qa.invoke(my_query)\n",
    "\n",
    "print(f\"Query: {my_response['query']}\\n\")\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "print(f\"Response: {my_response['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
